{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2NRnSljWZZg61hoqRYXrE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjscherrer/reinforcement-learning-tasks/blob/main/chapter_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.1\n",
        "\n",
        "\n",
        "*   Crypto Trading (States: [volatility, closing price, bid ask spread], Actions: [buy/sell/hold], Rewards: price in t+1)\n",
        "*   Tablut (States: [positions of figures, number of white/black figures], Actions: [set stone to field XY], Rewards: Number representing the quality of the play)\n",
        "*   Sports Training (States: [weight, nutrition], Actions: [set level of training intensity], Rewards: [performance improvement]) --> in this case, the state does not fulfill the Markov property as the state does not include information about all aspects of the past agent-environment interaction that make a difference for the future (e.g. previous training)\n",
        "\n"
      ],
      "metadata": {
        "id": "Khxbgzr4EHeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.2\n",
        "No it isn't. A clear exception would be world peace. One issue would be to define a numeric value for the reward representing world peace. Furthermore, the Markov property would be violated as we can't define a state including all information of past agent-environment interaction that make a difference for the future."
      ],
      "metadata": {
        "id": "SAzsw_UGJpzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.3\n",
        "The right place to draw the line between agent and environment is there, where the agent's absolute control over the things used to fulfill the task ends.\n",
        "In the example of driving (if the goal is moving the car safely), the line should be drawn at the actions which control the car directly (e.g. braking, steering). The mentioned example of considering where the rubber meets the road as actions would not be a good decision as this is outside of the agent's absolute control (e.g. temperature, road width). Furthermore, drawing the line at the choice of where to drive would not be a good decision because the mapping of actions and rewards would be impossible.\n",
        "\n",
        "A main point to consider is that many different agents may be operating at once, each with its own boundary. Therefore, depending on the goal that is important when driving a car, the problem could be separated into different agents."
      ],
      "metadata": {
        "id": "W37ZDxsvMMF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.4\n",
        "\n",
        "The rewards are deterministic (there is only one possible reward for each combination of $s$, $a$ and $s'$). Therefore, the dynamics table does look almost identical to the transition table (column names change).\n",
        "\n",
        "\\begin{array}{c c c c c}\n",
        "s    & a        & s'   & r          & p(s',r | s,a) \\\\\n",
        "high & search   & high & r_{search} & \\alpha        \\\\\n",
        "high & search   & low  & r_{search} & 1- \\alpha     \\\\\n",
        "low  & search   & high & -3         & 1-\\beta       \\\\\n",
        "low  & search   & low  & r_{search} & \\beta         \\\\\n",
        "high & wait     & high & r_{wait}   & 1             \\\\\n",
        "high & wait     & low  & -          & 0             \\\\\n",
        "low  & wait     & high & -          & 0             \\\\\n",
        "low  & wait     & low  & r_{wait}   & 1             \\\\\n",
        "low  & recharge & high & 0          & 1             \\\\\n",
        "low  & recharge & low  & -          & 0            \n",
        "\\end{array}"
      ],
      "metadata": {
        "id": "jfjTlwOS0AtL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.5\n",
        "We simply have to include the terminal state into the set of possible states $\\mathcal{S}^+$\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\sum_{s'\\ \\in \\ \\mathcal{S}^+}\\sum_{r\\ \\in \\ \\mathcal R} p(s',r|s,a) = 1, \\text{for all } s \\in \\mathcal{S^+}, a \\in \\mathcal{A(\\mathcal{S})}\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "sluMq_XvCtrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.6\n",
        "For both the episodic and continuing approach, we have to keep in mind the following explained on the episodic approach:\n",
        "The task fails at one specific time index $T$ (pole falls over) but the return $G_t$ can be calculated at each possible time step $t$. Therefore, in both cases we need to incorporate bot $T$ and $t$ to get the correct discount rates. Furthermore, $R_{t+1}$ does not get discounted.\n",
        "\n",
        "Episodic Approach: $G_t = -\\gamma^{T-t-1}$\n",
        "\n",
        "Continuing Approach: $G_t = \\sum_{\\mathcal{k} \\in \\mathcal{K}}-\\gamma^{\\mathcal k-t-1}$ where $\\mathcal{K}$ contains all time steps where the task failed"
      ],
      "metadata": {
        "id": "l7ZPN4-d87lR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.7\n",
        "The issue with this approach is that every single episode which leads to an escape from the maze gets the same return $G_t = +1$. Therefore, the algorithm won't learn to escape fast. It was not effectively communicated to the agent what we wanted to achieve. We should either take the time it took to escape into account to calculate the reward or the task should be reformulated in a continuous setting including discounting of rewards."
      ],
      "metadata": {
        "id": "NlMWjIVP8EJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.8\n",
        "$G_5 = 0$\n",
        "\n",
        "$G_4 = R_5 + \\gamma G_5 = 2$\n",
        "\n",
        "$G_3 = R_4 + \\gamma G_4 = 4$\n",
        "\n",
        "$G_2 = R_3 + \\gamma G_3 = 8$\n",
        "\n",
        "$G_1 = R_2 + \\gamma G_2 = 6$\n",
        "\n",
        "$G_0 = R_1 + \\gamma G_1 = 2$"
      ],
      "metadata": {
        "id": "gGkLjl2DD8XP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.9\n",
        "$G_0 = R_{t+1} + \\gamma G_{t+1} = R_{t+1} + R \\times \\sum^\\infty_{k=1}\\gamma^k = R_{t+1} + R \\times (\\frac{1}{1-\\gamma}-1) = 2 + 7 * 9 = 65$ with $R = 7$\n",
        "\n",
        "$G_1 = R \\times \\sum^\\infty_{k=0}\\gamma^k = R \\times \\frac{1}{1-\\gamma} = 7 * 10 = 70$ with $R = 7$"
      ],
      "metadata": {
        "id": "1A-1qgECGDpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.10\n",
        "First, we start by looking at the properties of a finite sum:\n",
        "\n",
        "$S_N = \\sum^N_{k=0}\\gamma^k = 1 + \\gamma + \\gamma^2 + ... + \\gamma^N = 1 + \\gamma(1 + \\gamma + \\gamma^2 + ... + \\gamma^{N-1}) = 1 + \\gamma S_{N-1}$\n",
        "\n",
        "If we consider the sum going to infinity: $S_\\infty \\approx S_{\\infty - 1}$\n",
        "\n",
        "$S_\\infty = 1 + \\gamma S_\\infty$\n",
        "\n",
        "Therefore:\n",
        "\n",
        "$S_\\infty = \\frac{1}{1-\\gamma}$"
      ],
      "metadata": {
        "id": "-8o6_9Z5OFtL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.11\n",
        "$E_\\pi[R_{t+1}|S_t] = \\sum_{\\forall r} r \\times p_\\pi(r|s_t) = \\sum_{\\forall a} \\pi(a|s) \\times \\sum_{\\forall s,\\forall s'} r \\times p(s', r | s, a) $"
      ],
      "metadata": {
        "id": "Ztdu-60hm1FT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.12\n",
        "$\\upsilon_\\pi(s) = \\sum_{\\forall a} q_\\pi(s,a) \\times \\pi(a|s)$"
      ],
      "metadata": {
        "id": "128JgBv9tiBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.13\n",
        "The current state $s$ and action $a$ are fixed. We have to loop over all possible next states $s'$ and all possible rewards $r$. Their probability is given by $p(s',r|s,a)$ while their value at time $t$ is given by $r + \\gamma \\upsilon_\\pi(s')$ (see equation 3.9 in book).\n",
        "\n",
        "$q_\\pi(s,a) = \\sum_{\\forall s', \\forall a} p(s',r|s,a) \\times (r + \\gamma \\upsilon_\\pi(s'))$"
      ],
      "metadata": {
        "id": "e7RO_f6nulaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.14\n",
        "We start by looking at the Bellman equation:\n",
        "\n",
        "$\\upsilon_\\pi(s) = \\sum_a \\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r + \\gamma \\upsilon_\\pi(s')]$\n",
        "\n",
        "What we can say about policy $\\pi$ is that each single action given the central state has the same probability of $\\frac{1}{4}$. Furthermore, $p(s',r|s,a) = 1$ for each combination, as there is only one possible next state $s'$ and reward $r = 0$ for a given state $s$ and action $a$. Therefore, the only thing that is different for each action chosen is the discounted value of the next state $s'$. The problem simplifies to the following:\n",
        "\n",
        "$\\upsilon_\\pi(s) = \\frac{1}{4} \\times 0.9 \\times (2.3 + 0.4 + -0.4 + 0.7) = 0.675 ≈ 0.7$"
      ],
      "metadata": {
        "id": "NNNPVP5TXT5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.15\n",
        "Starting with 3.8:\n",
        "$$\n",
        "\\begin{align}\n",
        "G_t &= \\sum^\\infty_{k=0} \\gamma^k(R_{t+k+1} + c) \\\\\n",
        "&= \\sum^\\infty_{k=0} \\gamma^k R_{t+k+1} + \\sum^\\infty_{k=0} \\gamma^kc \\\\\n",
        "&= \\sum^\\infty_{k=0} \\gamma^k R_{t+k+1} + \\frac{c}{1-\\gamma}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Thus:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\upsilon_\\pi(s) &= E_\\pi\\left[\\sum^\\infty_{k=0} \\gamma^k R_{t+k+1} + \\frac{c}{1-\\gamma}\\middle|S_t = s\\right] \\\\\n",
        "&= E_\\pi\\left[\\sum^\\infty_{k=0} \\gamma^k R_{t+k+1}\\middle|S_t = s\\right] + E_\\pi\\left[\\frac{c}{1-\\gamma}\\middle|S_t = s\\right] \\\\\n",
        "&= E_\\pi\\left[\\sum^\\infty_{k=0} \\gamma^k R_{t+k+1}\\middle|S_t = s\\right] + \\frac{c}{1-\\gamma}\\\\\n",
        "&= E_\\pi\\left[\\sum^\\infty_{k=0} \\gamma^k R_{t+k+1}\\middle|S_t = s\\right] + \\upsilon_c\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "umgi59ysH2Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.16\n",
        "Starting with an expression for $G_t$:\n",
        "$$\n",
        "\\begin{align}\n",
        "G_t &= \\sum^{T-t-1}_{k=0} (R_{t+k+1} + c) \\\\\n",
        "&= \\sum^{T-t-1}_{k=0} R_{t+k+1} + \\sum^{T-t-1}_{k=0} c \\\\\n",
        "&= \\sum^{T-t-1}_{k=0} R_{t+k+1} + (T-t)c\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Thus:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\upsilon_\\pi(s) &= E_\\pi\\left[\\sum^{T-t-1}_{k=0} R_{t+k+1} + Tc - tc\\middle|S_t = s\\right] \\\\\n",
        "&= E_\\pi\\left[\\sum^{T-t-1}_{k=0} \\gamma^k R_{t+k+1}\\middle|S_t = s\\right] + c \\times E_\\pi\\left[T\\middle|S_t = s\\right] - E_\\pi\\left[ct\\middle|S_t = s\\right] \\\\\n",
        "&= E_\\pi\\left[\\sum^{T-t-1}_{k=0} \\gamma^k R_{t+k+1}\\middle|S_t = s\\right] + c \\times E_\\pi\\left[T\\middle|S_t = s\\right] - ct\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Therefore, it would affect the relative values of the states. The higher $c$, the more the algorithm would favourise states which are far from the terminal state $T$. Obviously, this is not good as it could then take forever to get to the terminal state."
      ],
      "metadata": {
        "id": "QP7V8kX5Omvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3.17\n",
        "$$\n",
        "\\begin{align}\n",
        "q_\\pi(s,a) &= E_\\pi[G_t|S_t = s, A_t = a] \\\\\n",
        "&= \\sum_{s',r} p(s', r|s,a) \\times (r + \\gamma E_\\pi[G_{t+1}|S_{t+1}=s', A_{t+1} = a]) \\\\\n",
        "&= \\sum_{s',r} p(s', r|s,a) \\times (r + \\gamma \\sum_{a'} \\pi(a'|s') \\times q_\\pi(s',a'))\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "PAufLI_2nNpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3.18\n",
        "The first equation is given by:\n",
        "\n",
        "$\\upsilon_\\pi(s) = E_\\pi[q_\\pi(s,a)|S_t = s]$\n",
        "\n",
        "The second equation is given by:\n",
        "\n",
        "$υ_\\pi(s) = \\sum_a \\pi(a|s) \\times q_\\pi(s,a)$"
      ],
      "metadata": {
        "id": "1xP7XTpLZjHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3.19\n",
        "The first equation is given by:\n",
        "\n",
        "$q_\\pi(s,a) = E[R_{t+1} + \\gamma υ_π(s_{t+1})|S_t = s, A_t = a]$\n",
        "\n",
        "The second equation is given by:\n",
        "\n",
        "$υ_\\pi(s) = \\sum_{s',r} p(s', r|s,a) \\times (r + \\gamma \\upsilon_\\pi(s'))$"
      ],
      "metadata": {
        "id": "FavzBZXldx-n"
      }
    }
  ]
}