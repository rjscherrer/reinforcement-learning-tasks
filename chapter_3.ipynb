{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMye4hlmaE05QAztl/DXHgl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjscherrer/reinforcement-learning-tasks/blob/main/chapter_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.1\n",
        "\n",
        "\n",
        "*   Crypto Trading (States: [volatility, closing price, bid ask spread], Actions: [buy/sell/hold], Rewards: price in t+1)\n",
        "*   Tablut (States: [positions of figures, number of white/black figures], Actions: [set stone to field XY], Rewards: Number representing the quality of the play)\n",
        "*   Sports Training (States: [weight, nutrition], Actions: [set level of training intensity], Rewards: [performance improvement]) --> in this case, the state does not fulfill the Markov property as the state does not include information about all aspects of the past agent-environment interaction that make a difference for the future (e.g. previous training)\n",
        "\n"
      ],
      "metadata": {
        "id": "Khxbgzr4EHeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.2\n",
        "No it isn't. A clear exception would be world peace. One issue would be to define a numeric value for the reward representing world peace. Furthermore, the Markov property would be violated as we can't define a state including all information of past agent-environment interaction that make a difference for the future."
      ],
      "metadata": {
        "id": "SAzsw_UGJpzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.3\n",
        "The right place to draw the line between agent and environment is there, where the agent's absolute control over the things used to fulfill the task ends.\n",
        "In the example of driving (if the goal is moving the car safely), the line should be drawn at the actions which control the car directly (e.g. braking, steering). The mentioned example of considering where the rubber meets the road as actions would not be a good decision as this is outside of the agent's absolute control (e.g. temperature, road width). Furthermore, drawing the line at the choice of where to drive would not be a good decision because the mapping of actions and rewards would be impossible.\n",
        "\n",
        "A main point to consider is that many different agents may be operating at once, each with its own boundary. Therefore, depending on the goal that is important when driving a car, the problem could be separated into different agents."
      ],
      "metadata": {
        "id": "W37ZDxsvMMF2"
      }
    }
  ]
}